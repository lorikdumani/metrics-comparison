Id;PostId;UserId;PostHistoryTypeId;RevisionGUID;CreationDate;Text;UserDisplayName;Comment
11203021;5251258;652286;2;6cf03e3f-46da-433a-93d5-881d754f6bc4;2011-03-09 19:42:41.0;"Here is the problem I'm trying to solve:&#xD;&#xA;&#xD;&#xA;I have about 100 binary files (in total 158KB and they are roughly the same size +/- 50% of each other). I need to selectively parse only a few of these files (in the worst case maybe 50, in other cases as little as 1 to 5). This is on an Android device, by the way.&#xD;&#xA;&#xD;&#xA;*What is the fastest way to do this in Java?*&#xD;&#xA;&#xD;&#xA;One way could be combining everything into one file and then using file seek to get to the each individual file. That way file open would only need to be called once and that is usually slow. However, in order to know where each file is there would need to be some sort of table in the beginning of the file -- which could be generated using a script -- but the files would also need to be indexed in the table in the order that they were concatenated so file seek wouldn't have to do much work (correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;A better way would be to make the file memory-mapped and then the table wouldn't have to be in sorted order of concatenation because the memory-mapped file would have random access (again correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;Creating that table would be an unnecessary if zip compression was used because zip compression already makes a table. In addition, all the files wouldn't have to be concatenated. I could zip the directory and then access each of the individual files by their entries in the zip file. Problem solved.&#xD;&#xA;&#xD;&#xA;Except if the zip file isn't memory-mapped, it will be slower to read, since system calls are slower than direct memory access (correct me if I'm wrong). **So I came to the conclusion that the best solution would be to use a memory-mapped zip archive.**&#xD;&#xA;&#xD;&#xA;However, the ZipFile entries return an InputStream to read the contents of the entry. And the MappedByteBuffer needs a RandomAccessFile which takes a filename as input, not an InputStream.&#xD;&#xA;&#xD;&#xA;*Is there anyway to memory-map a zip file for fast reads? Or is there a different solution to this problem of reading a selection of files?*&#xD;&#xA;&#xD;&#xA;Thanks";;
11203707;5251258;652286;5;53b1d830-fc77-4d35-8330-90b1ca180181;2011-03-09 20:11:44.0;"Here is the problem I'm trying to solve:&#xD;&#xA;&#xD;&#xA;I have about 100 binary files (in total 158KB and they are roughly the same size +/- 50% of each other). I need to selectively parse only a few of these files (in the worst case maybe 50, in other cases as little as 1 to 5). This is on an Android device, by the way.&#xD;&#xA;&#xD;&#xA;*What is the fastest way to do this in Java?*&#xD;&#xA;&#xD;&#xA;One way could be combining everything into one file and then using file seek to get to the each individual file. That way file open would only need to be called once and that is usually slow. However, in order to know where each file is there would need to be some sort of table in the beginning of the file -- which could be generated using a script -- but the files would also need to be indexed in the table in the order that they were concatenated so file seek wouldn't have to do much work (correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;A better way would be to make the file memory-mapped and then the table wouldn't have to be in sorted order of concatenation because the memory-mapped file would have random access (again correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;Creating that table would be an unnecessary if zip compression was used because zip compression already makes a table. In addition, all the files wouldn't have to be concatenated. I could zip the directory and then access each of the individual files by their entries in the zip file. Problem solved.&#xD;&#xA;&#xD;&#xA;Except if the zip file isn't memory-mapped, it will be slower to read, since system calls are slower than direct memory access (correct me if I'm wrong). **So I came to the conclusion that the best solution would be to use a memory-mapped zip archive.**&#xD;&#xA;&#xD;&#xA;However, the `ZipFile` entries return an `InputStream` to read the contents of the entry. And the `MappedByteBuffer` needs a `RandomAccessFile` which takes a filename as input, not an `InputStream`.&#xD;&#xA;&#xD;&#xA;*Is there anyway to memory-map a zip file for fast reads? Or is there a different solution to this problem of reading a selection of files?*&#xD;&#xA;&#xD;&#xA;Thanks";;added 10 characters in body
11212987;5251258;652286;5;e79726f7-01ee-4a4b-a849-aa939fc90159;2011-03-10 04:58:28.0;"Here is the problem I'm trying to solve:&#xD;&#xA;&#xD;&#xA;I have about 100 binary files (in total 158KB and they are roughly the same size +/- 50% of each other). I need to selectively parse only a few of these files (in the worst case maybe 50, in other cases as little as 1 to 5). This is on an Android device, by the way.&#xD;&#xA;&#xD;&#xA;*What is the fastest way to do this in Java?*&#xD;&#xA;&#xD;&#xA;One way could be combining everything into one file and then using file seek to get to the each individual file. That way file open would only need to be called once and that is usually slow. However, in order to know where each file is there would need to be some sort of table in the beginning of the file -- which could be generated using a script -- but the files would also need to be indexed in the table in the order that they were concatenated so file seek wouldn't have to do much work (correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;A better way would be to make the file memory-mapped and then the table wouldn't have to be in sorted order of concatenation because the memory-mapped file would have random access (again correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;Creating that table would be an unnecessary if zip compression was used because zip compression already makes a table. In addition, all the files wouldn't have to be concatenated. I could zip the directory and then access each of the individual files by their entries in the zip file. Problem solved.&#xD;&#xA;&#xD;&#xA;Except if the zip file isn't memory-mapped, it will be slower to read, since system calls are slower than direct memory access (correct me if I'm wrong). **So I came to the conclusion that the best solution would be to use a memory-mapped zip archive.**&#xD;&#xA;&#xD;&#xA;However, the `ZipFile` entries return an `InputStream` to read the contents of the entry. And the `MappedByteBuffer` needs a `RandomAccessFile` which takes a filename as input, not an `InputStream`.&#xD;&#xA;&#xD;&#xA;*Is there anyway to memory-map a zip file for fast reads? Or is there a different solution to this problem of reading a selection of files?*&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;EDIT: I tested speeds of open, close, and parsing of the files here are the statistics that I found:&#xD;&#xA;&#xD;&#xA;`Number of Files: 25 (24 for parse because garbage collection interrupted timing)`  &#xD;&#xA;`Total Open Time: 72ms`  &#xD;&#xA;`Total Close Time: 1ms`  &#xD;&#xA;`Total Parse Time:  515ms `  &#xD;&#xA;&#xD;&#xA;(this is skewed in Parses favor because Parse is missing a file)  &#xD;&#xA;`%Total time Open takes: 12%`  &#xD;&#xA;`%Total time Close takes: 0.17%`  &#xD;&#xA;`%Total time Parse takes: 88%`  &#xD;&#xA;&#xD;&#xA;`Avg time Open takes per file: 2.88ms`  &#xD;&#xA;`Avg time Close takes per file: 0.04ms`  &#xD;&#xA;`Avg time Parse takes per file: 21.46ms`  ";;added 581 characters in body
11244326;5251258;652286;5;e0daa973-39a6-4a43-a186-717d052f9b48;2011-03-11 04:52:45.0;"Here is the problem I'm trying to solve:&#xD;&#xA;&#xD;&#xA;I have about 100 binary files (in total 158KB and they are roughly the same size +/- 50% of each other). I need to selectively parse only a few of these files (in the worst case maybe 50, in other cases as little as 1 to 5). This is on an Android device, by the way.&#xD;&#xA;&#xD;&#xA;*What is the fastest way to do this in Java?*&#xD;&#xA;&#xD;&#xA;One way could be combining everything into one file and then using file seek to get to the each individual file. That way file open would only need to be called once and that is usually slow. However, in order to know where each file is there would need to be some sort of table in the beginning of the file -- which could be generated using a script -- but the files would also need to be indexed in the table in the order that they were concatenated so file seek wouldn't have to do much work (correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;A better way would be to make the file memory-mapped and then the table wouldn't have to be in sorted order of concatenation because the memory-mapped file would have random access (again correct me if I'm wrong).&#xD;&#xA;&#xD;&#xA;Creating that table would be an unnecessary if zip compression was used because zip compression already makes a table. In addition, all the files wouldn't have to be concatenated. I could zip the directory and then access each of the individual files by their entries in the zip file. Problem solved.&#xD;&#xA;&#xD;&#xA;Except if the zip file isn't memory-mapped, it will be slower to read, since system calls are slower than direct memory access (correct me if I'm wrong). **So I came to the conclusion that the best solution would be to use a memory-mapped zip archive.**&#xD;&#xA;&#xD;&#xA;However, the `ZipFile` entries return an `InputStream` to read the contents of the entry. And the `MappedByteBuffer` needs a `RandomAccessFile` which takes a filename as input, not an `InputStream`.&#xD;&#xA;&#xD;&#xA;*Is there anyway to memory-map a zip file for fast reads? Or is there a different solution to this problem of reading a selection of files?*&#xD;&#xA;&#xD;&#xA;Thanks&#xD;&#xA;&#xD;&#xA;EDIT: I tested speeds of open, close, and parsing of the files here are the statistics that I found:&#xD;&#xA;&#xD;&#xA;`Number of Files: 25 (24 for parse because garbage collection interrupted timing)`  &#xD;&#xA;`Total Open Time: 72ms`  &#xD;&#xA;`Total Close Time: 1ms`  &#xD;&#xA;`Total Parse Time:  515ms `  &#xD;&#xA;&#xD;&#xA;(this is skewed in Parse's favor because Parse is missing a file)  &#xD;&#xA;`%Total time Open takes: 12%`  &#xD;&#xA;`%Total time Close takes: 0.17%`  &#xD;&#xA;`%Total time Parse takes: 88%`  &#xD;&#xA;&#xD;&#xA;`Avg time Open takes per file: 2.88ms`  &#xD;&#xA;`Avg time Close takes per file: 0.04ms`  &#xD;&#xA;`Avg time Parse takes per file: 21.46ms`  ";;added 1 characters in body
