Id;PostId;UserId;PostHistoryTypeId;RevisionGUID;CreationDate;Text;UserDisplayName;Comment
97103658;31930333;1101823;2;77321463-f326-4424-8aae-8e7d6a2903bb;2015-08-10 22:39:34.0;"Input splits are logical references to data. If you look at the [API][1], you can see that it doesn't know anything about the record boundaries. A mapper is launched for every input split. A mapper's `map()` is run for every record(In a WordCount program, every line in a file).&#xD;&#xA;&#xD;&#xA;But how does a mapper know where the record boundaries are?  &#xD;&#xA;&#xD;&#xA;This is where your quote from Hadoop MapReduce [InputFormat][2] Interface comes in -&#xD;&#xA;&#xD;&#xA;> the application has to also implement a RecordReader on whom lies the&#xD;&#xA;> responsibilty to respect record-boundaries and present a&#xD;&#xA;> record-oriented view of the logical InputSplit to the individual task&#xD;&#xA;&#xD;&#xA;Every mapper is associated with an InputFormat. That `InputFormat` has information on which `RecordReader` to use. Look at the [API][2], you will find that it knows about the input splits and what record reader to use.&#xD;&#xA;If you want to know some more about input splits and Record reader, you should read [this][3] answer.&#xD;&#xA;&#xD;&#xA;A `RecordReader` defines what the record boundaries are; The `InputFormat` defines what `RecordReader` is used.&#xD;&#xA;&#xD;&#xA;The WordCount program does not specify any `InputFormat`, it therefore defaults to `TextInputFormat` which uses [LineRecordReader][4] and gives out every line as a different record.&#xD;&#xA;&#xD;&#xA;Every MapReduce program 'respects' record boundaries. Otherwise, it is of not much use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputSplit.html&#xD;&#xA;  [2]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html&#xD;&#xA;  [3]: http://stackoverflow.com/a/14540272/1101823&#xD;&#xA;  [4]: https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java";;
97171970;31930333;1101823;5;cc11c277-91b9-49ce-a1a4-d262dda86189;2015-08-11 17:17:04.0;"Input splits are logical references to data. If you look at the [API][1], you can see that it doesn't know anything about the record boundaries. A mapper is launched for every input split. A mapper's `map()` is run for every record(In a WordCount program, every line in a file).&#xD;&#xA;&#xD;&#xA;But how does a mapper know where the record boundaries are?  &#xD;&#xA;&#xD;&#xA;This is where your quote from Hadoop MapReduce [InputFormat][2] Interface comes in -&#xD;&#xA;&#xD;&#xA;> the application has to also implement a RecordReader on whom lies the&#xD;&#xA;> responsibilty to respect record-boundaries and present a&#xD;&#xA;> record-oriented view of the logical InputSplit to the individual task&#xD;&#xA;&#xD;&#xA;Every mapper is associated with an InputFormat. That `InputFormat` has information on which `RecordReader` to use. Look at the [API][2], you will find that it knows about the input splits and what record reader to use.&#xD;&#xA;If you want to know some more about input splits and Record reader, you should read [this][3] answer.&#xD;&#xA;&#xD;&#xA;A `RecordReader` defines what the record boundaries are; The `InputFormat` defines what `RecordReader` is used.&#xD;&#xA;&#xD;&#xA;The WordCount program does not specify any `InputFormat`, it therefore defaults to `TextInputFormat` which uses [LineRecordReader][4] and gives out every line as a different record.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;> [L]ogical splits based on input-size is insufficient for many&#xD;&#xA;> applications since record boundaries are to be respected.&#xD;&#xA;&#xD;&#xA;What this means is that, for an example file such as&#xD;&#xA;&#xD;&#xA;    a b c d e&#xD;&#xA;    f g h i j&#xD;&#xA;    k l m n o&#xD;&#xA;and we want every line to be a record. when the logical splits are based on input size, it is possible there may be two splits such as:&#xD;&#xA;&#xD;&#xA;    a b c d e&#xD;&#xA;    f g &#xD;&#xA;and &#xD;&#xA;&#xD;&#xA;        h i j &#xD;&#xA;    k l m n 0 &#xD;&#xA;&#xD;&#xA;If it wasn't for the `RecordReader`, it would've considered `f g ` and `h i j` to be different records; Clearly, this isn't what most applications want.&#xD;&#xA;&#xD;&#xA;Answering your question, in the WordCount program, it does not really matter what the record boundaries are but there is a possibility that the same word is split into different logical splits. Therefore, logical splits based on size are not sufficient for WordCount program.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Every MapReduce program 'respects' record boundaries. Otherwise, it is of not much use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputSplit.html&#xD;&#xA;  [2]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html&#xD;&#xA;  [3]: http://stackoverflow.com/a/14540272/1101823&#xD;&#xA;  [4]: https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java";;question changed, answer is changed to reflect the question
97430070;31930333;1101823;5;a1435c2b-5a31-4b91-b740-58e190f43231;2015-08-14 17:23:52.0;"Input splits are logical references to data. If you look at the [API][1], you can see that it doesn't know anything about the record boundaries. A mapper is launched for every input split. A mapper's `map()` is run for every record(In a WordCount program, every line in a file).&#xD;&#xA;&#xD;&#xA;But how does a mapper know where the record boundaries are?  &#xD;&#xA;&#xD;&#xA;This is where your quote from Hadoop MapReduce [InputFormat][2] Interface comes in -&#xD;&#xA;&#xD;&#xA;> the application has to also implement a RecordReader on whom lies the&#xD;&#xA;> responsibilty to respect record-boundaries and present a&#xD;&#xA;> record-oriented view of the logical InputSplit to the individual task&#xD;&#xA;&#xD;&#xA;Every mapper is associated with an InputFormat. That `InputFormat` has information on which `RecordReader` to use. Look at the [API][2], you will find that it knows about the input splits and what record reader to use.&#xD;&#xA;If you want to know some more about input splits and Record reader, you should read [this][3] answer.&#xD;&#xA;&#xD;&#xA;A `RecordReader` defines what the record boundaries are; The `InputFormat` defines what `RecordReader` is used.&#xD;&#xA;&#xD;&#xA;The WordCount program does not specify any `InputFormat`, it therefore defaults to `TextInputFormat` which uses [LineRecordReader][4] and gives out every line as a different record. And [this][4] your source code&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;> [L]ogical splits based on input-size is insufficient for many&#xD;&#xA;> applications since record boundaries are to be respected.&#xD;&#xA;&#xD;&#xA;What this means is that, for an example file such as&#xD;&#xA;&#xD;&#xA;    a b c d e&#xD;&#xA;    f g h i j&#xD;&#xA;    k l m n o&#xD;&#xA;and we want every line to be a record. when the logical splits are based on input size, it is possible there may be two splits such as:&#xD;&#xA;&#xD;&#xA;    a b c d e&#xD;&#xA;    f g &#xD;&#xA;and &#xD;&#xA;&#xD;&#xA;        h i j &#xD;&#xA;    k l m n 0 &#xD;&#xA;&#xD;&#xA;If it wasn't for the `RecordReader`, it would've considered `f g ` and `h i j` to be different records; Clearly, this isn't what most applications want.&#xD;&#xA;&#xD;&#xA;Answering your question, in the WordCount program, it does not really matter what the record boundaries are but there is a possibility that the same word is split into different logical splits. Therefore, logical splits based on size are not sufficient for WordCount program.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Every MapReduce program 'respects' record boundaries. Otherwise, it is of not much use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputSplit.html&#xD;&#xA;  [2]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html&#xD;&#xA;  [3]: http://stackoverflow.com/a/14540272/1101823&#xD;&#xA;  [4]: https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java";;added link
146594450;31930333;-1;5;305e5fa1-4af2-4915-b723-01c17a2983ed;2017-05-23 11:52:56.0;"Input splits are logical references to data. If you look at the [API][1], you can see that it doesn't know anything about the record boundaries. A mapper is launched for every input split. A mapper's `map()` is run for every record(In a WordCount program, every line in a file).&#xD;&#xA;&#xD;&#xA;But how does a mapper know where the record boundaries are?  &#xD;&#xA;&#xD;&#xA;This is where your quote from Hadoop MapReduce [InputFormat][2] Interface comes in -&#xD;&#xA;&#xD;&#xA;> the application has to also implement a RecordReader on whom lies the&#xD;&#xA;> responsibilty to respect record-boundaries and present a&#xD;&#xA;> record-oriented view of the logical InputSplit to the individual task&#xD;&#xA;&#xD;&#xA;Every mapper is associated with an InputFormat. That `InputFormat` has information on which `RecordReader` to use. Look at the [API][2], you will find that it knows about the input splits and what record reader to use.&#xD;&#xA;If you want to know some more about input splits and Record reader, you should read [this][3] answer.&#xD;&#xA;&#xD;&#xA;A `RecordReader` defines what the record boundaries are; The `InputFormat` defines what `RecordReader` is used.&#xD;&#xA;&#xD;&#xA;The WordCount program does not specify any `InputFormat`, it therefore defaults to `TextInputFormat` which uses [LineRecordReader][4] and gives out every line as a different record. And [this][4] your source code&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;----------&#xD;&#xA;> [L]ogical splits based on input-size is insufficient for many&#xD;&#xA;> applications since record boundaries are to be respected.&#xD;&#xA;&#xD;&#xA;What this means is that, for an example file such as&#xD;&#xA;&#xD;&#xA;    a b c d e&#xD;&#xA;    f g h i j&#xD;&#xA;    k l m n o&#xD;&#xA;and we want every line to be a record. when the logical splits are based on input size, it is possible there may be two splits such as:&#xD;&#xA;&#xD;&#xA;    a b c d e&#xD;&#xA;    f g &#xD;&#xA;and &#xD;&#xA;&#xD;&#xA;        h i j &#xD;&#xA;    k l m n 0 &#xD;&#xA;&#xD;&#xA;If it wasn't for the `RecordReader`, it would've considered `f g ` and `h i j` to be different records; Clearly, this isn't what most applications want.&#xD;&#xA;&#xD;&#xA;Answering your question, in the WordCount program, it does not really matter what the record boundaries are but there is a possibility that the same word is split into different logical splits. Therefore, logical splits based on size are not sufficient for WordCount program.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;Every MapReduce program 'respects' record boundaries. Otherwise, it is of not much use.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputSplit.html&#xD;&#xA;  [2]: https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html&#xD;&#xA;  [3]: https://stackoverflow.com/a/14540272/1101823&#xD;&#xA;  [4]: https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/LineRecordReader.java";;replaced http://stackoverflow.com/ with https://stackoverflow.com/
