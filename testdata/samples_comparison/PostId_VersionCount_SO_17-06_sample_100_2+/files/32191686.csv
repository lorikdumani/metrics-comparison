Id;PostId;UserId;PostHistoryTypeId;RevisionGUID;CreationDate;Text;UserDisplayName;Comment
98128650;32191686;1030820;2;7b0d21bb-6d40-46b6-94eb-de07c47799f1;2015-08-24 21:02:44.0;"Try to divide your data or load it by batches into script, and fit your PCA with something like [Incremetal PCA][1] and it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA";;
98130659;32191686;1030820;5;322a1b9e-a832-4e39-80de-312ab95b8d04;2015-08-24 21:43:43.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] and it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA";;deleted 15 characters in body
98146278;32191686;1030820;5;19204202-62a6-4635-8b6a-8e08d9130e80;2015-08-25 05:35:04.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] and it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;    from sklearn.decomposition import IncrementalPCA&#xD;&#xA;    import csv&#xD;&#xA;    import sys&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas as pd&#xD;&#xA;    &#xD;&#xA;    dataset = sys.argv[1]&#xD;&#xA;    chunksize_ = 5 * 25000&#xD;&#xA;    dimensions = 300&#xD;&#xA;    &#xD;&#xA;    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)&#xD;&#xA;    sklearn_pca = IncrementalPCA(n_components=dimensions)&#xD;&#xA;    for chunk in reader:&#xD;&#xA;        y = chunk.pop(""virginica"")&#xD;&#xA;        sklearn_pca.partial_fit(chunk)&#xD;&#xA;    &#xD;&#xA;    # Computed mean per feature&#xD;&#xA;    mean = sklearn_pca.mean_&#xD;&#xA;    # and stddev&#xD;&#xA;    stddev = np.sqrt(sklearn_pca.var_)&#xD;&#xA;&#xD;&#xA;If you want to normalize data before pca you can just create second pca model and fit it in the same manner, but normalize chunks with computed mean and stddev from previous pca `sklearn_pca2.partial_fit((chunk-mean)/stddev)`&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA&#xD;&#xA;";;added 815 characters in body
98146901;32191686;1030820;5;58123320-662a-4c17-9f8d-9284cb0a3119;2015-08-25 05:47:20.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] and it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;    from sklearn.decomposition import IncrementalPCA&#xD;&#xA;    import csv&#xD;&#xA;    import sys&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas as pd&#xD;&#xA;    &#xD;&#xA;    dataset = sys.argv[1]&#xD;&#xA;    chunksize_ = 5 * 25000&#xD;&#xA;    dimensions = 300&#xD;&#xA;    &#xD;&#xA;    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)&#xD;&#xA;    sklearn_pca = IncrementalPCA(n_components=dimensions)&#xD;&#xA;    for chunk in reader:&#xD;&#xA;        y = chunk.pop(""virginica"")&#xD;&#xA;        sklearn_pca.partial_fit(chunk)&#xD;&#xA;    &#xD;&#xA;    # Computed mean per feature&#xD;&#xA;    mean = sklearn_pca.mean_&#xD;&#xA;    # and stddev&#xD;&#xA;    stddev = np.sqrt(sklearn_pca.var_)&#xD;&#xA;&#xD;&#xA;If you want to normalize data before pca you can just create second pca model and fit it in the same manner, but normalize chunks with computed mean and stddev from previous pca `sklearn_pca2.partial_fit((chunk-mean)/stddev)`&#xD;&#xA;&#xD;&#xA;[Useful link][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA&#xD;&#xA;  [2]: https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas";;added 106 characters in body
98209807;32191686;1030820;5;8919faed-2939-45f4-a5ed-1818e80e9ed5;2015-08-25 19:52:54.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] and it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;    from sklearn.decomposition import IncrementalPCA&#xD;&#xA;    import csv&#xD;&#xA;    import sys&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas as pd&#xD;&#xA;    &#xD;&#xA;    dataset = sys.argv[1]&#xD;&#xA;    chunksize_ = 5 * 25000&#xD;&#xA;    dimensions = 300&#xD;&#xA;    &#xD;&#xA;    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)&#xD;&#xA;    sklearn_pca = IncrementalPCA(n_components=dimensions)&#xD;&#xA;    for chunk in reader:&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        sklearn_pca.partial_fit(chunk)&#xD;&#xA;    &#xD;&#xA;    # Computed mean per feature&#xD;&#xA;    mean = sklearn_pca.mean_&#xD;&#xA;    # and stddev&#xD;&#xA;    stddev = np.sqrt(sklearn_pca.var_)&#xD;&#xA;&#xD;&#xA;If you want to normalize data before pca you can just create second pca model and fit it in the same manner, but normalize chunks with computed mean and stddev from previous pca `sklearn_pca2.partial_fit((chunk-mean)/stddev)`&#xD;&#xA;&#xD;&#xA;[Useful link][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA&#xD;&#xA;  [2]: https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas";;deleted 8 characters in body
98211027;32191686;1030820;5;a26eb2fe-6a59-47f8-b2e1-6c835e382c57;2015-08-25 20:12:27.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] and it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;    from sklearn.decomposition import IncrementalPCA&#xD;&#xA;    import csv&#xD;&#xA;    import sys&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas as pd&#xD;&#xA;    &#xD;&#xA;    dataset = sys.argv[1]&#xD;&#xA;    chunksize_ = 5 * 25000&#xD;&#xA;    dimensions = 300&#xD;&#xA;    &#xD;&#xA;    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)&#xD;&#xA;    sklearn_pca = IncrementalPCA(n_components=dimensions)&#xD;&#xA;    for chunk in reader:&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        sklearn_pca.partial_fit(chunk)&#xD;&#xA;    &#xD;&#xA;    # Computed mean per feature&#xD;&#xA;    mean = sklearn_pca.mean_&#xD;&#xA;    # and stddev&#xD;&#xA;    stddev = np.sqrt(sklearn_pca.var_)&#xD;&#xA;&#xD;&#xA;    Xtransformed = None&#xD;&#xA;    for chunk in pd.read_csv(dataset, sep = ',', chunksize = chunksize_):&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        Xchunk = sklearn_pca.transform(chunk)&#xD;&#xA;        if Xtransformed == None:&#xD;&#xA;            Xtransformed = Xchunk&#xD;&#xA;        else:&#xD;&#xA;            Xtransformed = np.vstack((Xtransformed, Xchunk))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If you want to normalize data before pca you can just create second pca model and fit it in the same manner, but normalize chunks with computed mean and stddev from previous pca `sklearn_pca2.partial_fit((chunk-mean)/stddev)`&#xD;&#xA;&#xD;&#xA;[Useful link][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA&#xD;&#xA;  [2]: https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas";;added 270 characters in body
98464572;32191686;1030820;5;dae4064a-5d1b-48d8-9dda-e538b513eb51;2015-08-28 21:10:28.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] with it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;    from sklearn.decomposition import IncrementalPCA&#xD;&#xA;    import csv&#xD;&#xA;    import sys&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas as pd&#xD;&#xA;    &#xD;&#xA;    dataset = sys.argv[1]&#xD;&#xA;    chunksize_ = 5 * 25000&#xD;&#xA;    dimensions = 300&#xD;&#xA;    &#xD;&#xA;    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)&#xD;&#xA;    sklearn_pca = IncrementalPCA(n_components=dimensions)&#xD;&#xA;    for chunk in reader:&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        sklearn_pca.partial_fit(chunk)&#xD;&#xA;    &#xD;&#xA;    # Computed mean per feature&#xD;&#xA;    mean = sklearn_pca.mean_&#xD;&#xA;    # and stddev&#xD;&#xA;    stddev = np.sqrt(sklearn_pca.var_)&#xD;&#xA;&#xD;&#xA;    Xtransformed = None&#xD;&#xA;    for chunk in pd.read_csv(dataset, sep = ',', chunksize = chunksize_):&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        Xchunk = sklearn_pca.transform(chunk)&#xD;&#xA;        if Xtransformed == None:&#xD;&#xA;            Xtransformed = Xchunk&#xD;&#xA;        else:&#xD;&#xA;            Xtransformed = np.vstack((Xtransformed, Xchunk))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;If you want to normalize data before pca you can just create second pca model and fit it in the same manner, but normalize chunks with computed mean and stddev from previous pca `sklearn_pca2.partial_fit((chunk-mean)/stddev)`&#xD;&#xA;&#xD;&#xA;[Useful link][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA&#xD;&#xA;  [2]: https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas";;added 1 character in body
101943982;32191686;1030820;5;39bcc825-3cfd-47ae-a968-4f891bca914a;2015-10-16 05:12:06.0;"Try to divide your data or load it by batches into script, and fit your PCA with [Incremetal PCA][1] with it's *partial_fit* method on every batch.&#xD;&#xA;&#xD;&#xA;    from sklearn.decomposition import IncrementalPCA&#xD;&#xA;    import csv&#xD;&#xA;    import sys&#xD;&#xA;    import numpy as np&#xD;&#xA;    import pandas as pd&#xD;&#xA;    &#xD;&#xA;    dataset = sys.argv[1]&#xD;&#xA;    chunksize_ = 5 * 25000&#xD;&#xA;    dimensions = 300&#xD;&#xA;    &#xD;&#xA;    reader = pd.read_csv(dataset, sep = ',', chunksize = chunksize_)&#xD;&#xA;    sklearn_pca = IncrementalPCA(n_components=dimensions)&#xD;&#xA;    for chunk in reader:&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        sklearn_pca.partial_fit(chunk)&#xD;&#xA;    &#xD;&#xA;    # Computed mean per feature&#xD;&#xA;    mean = sklearn_pca.mean_&#xD;&#xA;    # and stddev&#xD;&#xA;    stddev = np.sqrt(sklearn_pca.var_)&#xD;&#xA;&#xD;&#xA;    Xtransformed = None&#xD;&#xA;    for chunk in pd.read_csv(dataset, sep = ',', chunksize = chunksize_):&#xD;&#xA;        y = chunk.pop(""Y"")&#xD;&#xA;        Xchunk = sklearn_pca.transform(chunk)&#xD;&#xA;        if Xtransformed == None:&#xD;&#xA;            Xtransformed = Xchunk&#xD;&#xA;        else:&#xD;&#xA;            Xtransformed = np.vstack((Xtransformed, Xchunk))&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;[Useful link][2]&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA&#xD;&#xA;  [2]: https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas";;deleted 227 characters in body
